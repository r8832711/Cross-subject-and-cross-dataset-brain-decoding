{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading BOLD5000 data\n",
    "\n",
    "BOLD_brain_signals_train = torch.load(\"/BOLD5000/data_augmentation/train_25percent_noise_75.pt\", map_location=device)\n",
    "# BOLD_brain_signals_train = torch.load(\"/BOLD5000_V2/brain_signals_train.pt\", map_location=device)\n",
    "BOLD_brain_signals_test = torch.load(\"/BOLD5000_V2/brain_signals_test.pt\", map_location=device)\n",
    "\n",
    "# BOLD_brain_signals_train = torch.from_numpy(BOLD_brain_signals_train).to(device)\n",
    "BOLD_brain_signals_test = torch.from_numpy(BOLD_brain_signals_test).to(device)\n",
    "\n",
    "# Normalizing the BOLD signals\n",
    "\n",
    "mean = BOLD_brain_signals_train.mean(0)\n",
    "std = BOLD_brain_signals_train.std(0)\n",
    "BOLD_brain_signals_train = (BOLD_brain_signals_train - mean) / std\n",
    "BOLD_brain_signals_test = (BOLD_brain_signals_test - mean) / std\n",
    "\n",
    "BOLD_brain_signals_train = torch.nan_to_num(BOLD_brain_signals_train)\n",
    "BOLD_brain_signals_test = torch.nan_to_num(BOLD_brain_signals_test)\n",
    "\n",
    "\n",
    "BOLD_stimulus_embeddings_train = torch.load( \"/BOLD5000/data_augmentation/train_embds_25percent.pt\", map_location=device)\n",
    "# BOLD_stimulus_embeddings_train = torch.load( \"/BOLD5000_V2/image_embeddings_train.pt\", map_location=device)\n",
    "BOLD_stimulus_embeddings_test = torch.load(\"/BOLD5000_V2/image_embeddings_test.pt\", map_location=device)\n",
    "\n",
    "BOLD_subject_ids_train = torch.load(\"/BOLD5000/data_augmentation/train_ids_25percent.pt\", map_location=device)\n",
    "# BOLD_subject_ids_train = torch.load(\"/BOLD5000_V2/subject_ids_train.pt\", map_location=device)\n",
    "BOLD_subject_ids_train = torch.tensor(BOLD_subject_ids_train).to(device)\n",
    "\n",
    "BOLD_subject_ids_test = torch.load(\"/BOLD5000_V2/subject_ids_test.pt\", map_location=device)\n",
    "\n",
    "# Convert BOLD5000 brain signals to tensor\n",
    "BOLD_brain_signals_train = torch.tensor(BOLD_brain_signals_train).float()\n",
    "BOLD_brain_signals_test = torch.tensor(BOLD_brain_signals_test).float()\n",
    "\n",
    "# Extract CLIP embeddings from BOLD5000\n",
    "BOLD_stimulus_embeddings_train = BOLD_stimulus_embeddings_train[:, 1, 0].float()\n",
    "BOLD_stimulus_embeddings_test = BOLD_stimulus_embeddings_test[:, 1, 0].float()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([21566, 15724]),\n",
       " torch.Size([21566, 1280]),\n",
       " torch.Size([21566]),\n",
       " torch.Size([445, 15724]),\n",
       " torch.Size([445, 1280]),\n",
       " torch.Size([445]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOLD_brain_signals_train.shape, BOLD_stimulus_embeddings_train.shape, BOLD_subject_ids_train.shape, BOLD_brain_signals_test.shape, BOLD_stimulus_embeddings_test.shape, BOLD_subject_ids_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading NSD dataset\n",
    "\n",
    "NSD_train_data = torch.load(\"/decoding_NSD/data_augmentation/train_25percent_noise_50.pt\").to(device).float()\n",
    "\n",
    "NSD_test_data = np.load(\"/decoding_NSD/data_fmri_nsd/test_data.npy\")\n",
    "NSD_test_data = torch.from_numpy(NSD_test_data).to(device).float()\n",
    "\n",
    "\n",
    "mean = NSD_train_data.mean(0)\n",
    "std = NSD_train_data.std(0)\n",
    "NSD_train_data = (NSD_train_data - mean) / std\n",
    "NSD_test_data = (NSD_test_data - mean) / std\n",
    "\n",
    "NSD_train_data = torch.nan_to_num(NSD_train_data)\n",
    "NSD_test_data = torch.nan_to_num(NSD_test_data)\n",
    "\n",
    "NSD_train_img_embeds = torch.load(\"/decoding_NSD/data_augmentation/train_embds_25percent.pt\", map_location=device)\n",
    "NSD_test_img_embeds = torch.load(\"/decoding_NSD/data_fmri_nsd/test_clip_img_embeds.pt\", map_location=device)\n",
    "\n",
    "NSD_train_img_embeds = NSD_train_img_embeds[:, 1, 0].float()\n",
    "NSD_test_img_embeds = NSD_test_img_embeds[:, 1, 0].float()\n",
    "\n",
    "\n",
    "NSD_subject_train_ids = torch.load(\"/decoding_NSD/data_augmentation/train_ids_25percent.pt\")\n",
    "NSD_subject_test_ids = np.load(\"/decoding_NSD/data_fmri_nsd/subject_test_ids.npy\")\n",
    "\n",
    "# NSD_subject_train_ids=[int(i[-1]) for i in NSD_subject_train_ids]\n",
    "NSD_subject_test_ids=[int(i[-1]) for i in NSD_subject_test_ids]\n",
    "\n",
    "NSD_subject_train_ids = torch.from_numpy(np.array(NSD_subject_train_ids)).to(device)\n",
    "NSD_subject_test_ids = torch.from_numpy(np.array(NSD_subject_test_ids)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6000, 15724]),\n",
       " torch.Size([6000, 1280]),\n",
       " torch.Size([6000]),\n",
       " torch.Size([250, 15724]),\n",
       " torch.Size([250, 1280]),\n",
       " torch.Size([250]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading GOD dataset\n",
    "GOD_train_brain_signals = torch.load('/GOD/train_brain_signals.pt').to(device)\n",
    "GOD_test_brain_signals = torch.load('/GOD/test_brain_signals.pt').to(device)\n",
    "\n",
    "mean = GOD_train_brain_signals.mean(0)\n",
    "std = GOD_train_brain_signals.std(0)\n",
    "GOD_train_brain_signals = (GOD_train_brain_signals - mean) / std\n",
    "GOD_test_brain_signals = (GOD_test_brain_signals - mean) / std\n",
    "\n",
    "GOD_train_brain_signals = torch.nan_to_num(GOD_train_brain_signals).float()\n",
    "GOD_test_brain_signals = torch.nan_to_num(GOD_test_brain_signals).float()\n",
    "\n",
    "GOD_train_image_embds = torch.load('/GOD/train_image_embeddings.pt', map_location=device)\n",
    "GOD_test_image_embds = torch.load('/GOD/test_image_embeddings.pt', map_location=device)\n",
    "\n",
    "GOD_train_image_embds = GOD_train_image_embds[:, 1, 0].float()\n",
    "GOD_test_image_embds = GOD_test_image_embds[:, 1, 0].float()\n",
    "\n",
    "\n",
    "GOD_train_subject_ids = torch.load('/GOD/train_subject_ids.pt', map_location=device)\n",
    "# GOD_train_subject_ids = torch.tensor(GOD_train_subject_ids).to(device)\n",
    "GOD_test_subject_ids = torch.load('/GOD/test_subject_ids.pt', map_location=device)\n",
    "\n",
    "\n",
    "GOD_train_brain_signals.shape, GOD_train_image_embds.shape, GOD_train_subject_ids.shape, GOD_test_brain_signals.shape, GOD_test_image_embds.shape, GOD_test_subject_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate datasets\n",
    "cross_train_data = torch.cat([NSD_train_data, BOLD_brain_signals_train, GOD_train_brain_signals], dim=0)\n",
    "cross_test_data = torch.cat([NSD_test_data, BOLD_brain_signals_test, GOD_test_brain_signals], dim=0)\n",
    "\n",
    "cross_train_embeds = torch.cat([NSD_train_img_embeds, BOLD_stimulus_embeddings_train, GOD_train_image_embds], dim=0)\n",
    "cross_test_embeds = torch.cat([NSD_test_img_embeds, BOLD_stimulus_embeddings_test, GOD_test_image_embds], dim=0)\n",
    "\n",
    "cross_train_subjects = torch.cat([torch.tensor(NSD_subject_train_ids,  device=device), torch.tensor(BOLD_subject_ids_train, device=device), torch.tensor(GOD_train_subject_ids, device=device)], dim=0)\n",
    "cross_test_subjects = torch.cat([torch.tensor(NSD_subject_test_ids, device=device), torch.tensor(BOLD_subject_ids_test, device=device), torch.tensor(GOD_test_subject_ids, device=device)], dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim, act_fn=nn.ReLU, alignment_layers_keys=[12, 13, 14, 15, 16], common_dim=1024): #,  1, 2, 5, 7,  8, 9, 10, 11\n",
    "        super(Encoder, self).__init__()\n",
    "        self.common_dim = common_dim\n",
    "        self.alignment_layers = nn.ModuleDict({str(k): nn.Linear(input_dim, common_dim) for k in alignment_layers_keys})\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "            # Transformer Encoder\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=common_dim, nhead=8, dim_feedforward=2048, activation='gelu'), \n",
    "            num_layers=4\n",
    "        )\n",
    "        \n",
    "        layers = [nn.LayerNorm(common_dim)]\n",
    "        prev_dim = common_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.LayerNorm(hidden_dim))\n",
    "            layers.append(act_fn())\n",
    "            layers.append(nn.Linear(hidden_dim, prev_dim)) #Residual Connections \n",
    "            prev_dim = hidden_dim\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        layers.append(nn.LayerNorm(output_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "                # Alignment layer\n",
    "        self.alignment_layer = nn.Linear(output_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, k=None):\n",
    "        if k is None:\n",
    "            k = torch.ones(len(x), dtype=torch.long)\n",
    "        \n",
    "        result = torch.zeros(len(x), self.common_dim, device=x.device)\n",
    "        for key in k.unique():\n",
    "            mask = (k == key)\n",
    "            result[mask] = self.alignment_layers[str(key.item())](x[mask])\n",
    "            # result[mask] = self.dropout(result[mask]) #add dropoutout\n",
    "        #  # Add Transformer encoding here\n",
    "        result = result.unsqueeze(1)  # Add sequence dimension for Transformer (batch, seq_len, features)\n",
    "        result = self.transformer(result)\n",
    "        result = result.squeeze(1)  # Remove sequence dimension\n",
    "            #Alignment layer\n",
    "        aligned_result = self.net(result)\n",
    "        aligned_result = self.alignment_layer(aligned_result)\n",
    "        return aligned_result #self.net(result)\n",
    "\n",
    "class ContrastiveModel(pl.LightningModule):\n",
    "    def __init__(self, num_input_channels, base_channel_size, latent_dim, temperature=0.1, act_fn=nn.GELU, loss_type=\"contrastive\"):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.model = Encoder(num_input_channels, base_channel_size, latent_dim, act_fn)\n",
    "        self.loss_type = loss_type\n",
    "\n",
    "        if loss_type == \"contrastive\":\n",
    "            self.loss_fn = self.contrastive_loss\n",
    "        elif loss_type == \"mean_contrastive\":\n",
    "            self.loss_fn = self.mean_contrastive\n",
    "        elif loss_type == \"mse\":\n",
    "            self.loss_fn = torch.nn.functional.mse_loss\n",
    "        elif loss_type == \"cosine\":\n",
    "            self.loss_fn = self.cosine_loss\n",
    "\n",
    "        self.train_losses = []\n",
    "        self.train_mse = []\n",
    "        self.train_cosine = []\n",
    "        self.val_losses = []\n",
    "        self.val_mse = []\n",
    "        self.val_cosine = []\n",
    "\n",
    "        self.train_history = {\n",
    "            \"train_loss\": [],\n",
    "            \"train_mse\": [],\n",
    "            \"train_cosine\": []\n",
    "        }\n",
    "        \n",
    "        self.val_history = {\n",
    "            \"val_loss\": [],\n",
    "            \"val_mse\": [],\n",
    "            \"val_cosine\": []\n",
    "        }\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.model(x, **kwargs)\n",
    " \n",
    "    def contrastive_loss(self, z_i, z_j):\n",
    "        z_i = nn.functional.normalize(z_i, dim=1)\n",
    "        z_j = nn.functional.normalize(z_j, dim=1)\n",
    "        \n",
    "        logits = (z_i @ z_j.T) / self.temperature\n",
    "        targets = torch.arange(logits.shape[0]).long().to(logits.device)\n",
    "        \n",
    "        # Original cross-entropy loss\n",
    "        loss1 = torch.nn.functional.cross_entropy(logits, targets)\n",
    "        \n",
    "        # Transposed cross-entropy loss\n",
    "        loss2 = torch.nn.functional.cross_entropy(logits.T, targets)\n",
    "        \n",
    "        # Combined loss\n",
    "        loss = 0.5 * loss1 + 0.5 * loss2\n",
    "    \n",
    "        return loss\n",
    "\n",
    "\n",
    "    def mean_contrastive(self, z_i, z_j, temperature=1.0):\n",
    "        return nn.functional.mse_loss(z_i, z_j) + self.contrastive_loss(z_i, z_j, temperature=temperature) / 8\n",
    "    \n",
    "    def cosine_loss(self, z_i, z_j, temperature=1.0):\n",
    "        cosine_similarity = torch.nn.functional.cosine_similarity(z_i, z_j).mean()\n",
    "        return 1 - cosine_similarity\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y, idx = batch #add noise inX\n",
    "\n",
    "        noise = 0.15 * torch.randn_like(x) \n",
    "        x = x + noise \n",
    "\n",
    "        y_hat = self(x, k=idx)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self.log('train_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.train_losses.append(loss.item())\n",
    "\n",
    "        mse_loss = torch.nn.functional.mse_loss(y_hat, y)\n",
    "        cosine_similarity = torch.nn.functional.cosine_similarity(y_hat, y).mean()\n",
    "        self.train_mse.append(mse_loss.item())\n",
    "        self.train_cosine.append(cosine_similarity.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y, idx = batch\n",
    "        y_hat = self(x, k=idx)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        mse_loss = torch.nn.functional.mse_loss(y_hat, y)\n",
    "        self.log('val_mse_loss', mse_loss, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        cosine_similarity = torch.nn.functional.cosine_similarity(y_hat, y).mean()\n",
    "        self.log('val_cosine_similarity', cosine_similarity, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        self.val_losses.append(loss.item())\n",
    "        self.val_mse.append(mse_loss.item())\n",
    "        self.val_cosine.append(cosine_similarity.item())\n",
    "        return mse_loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self.train_history[\"train_loss\"].append(np.mean(self.train_losses))\n",
    "        self.train_history[\"train_mse\"].append(np.mean(self.train_mse))\n",
    "        self.train_history[\"train_cosine\"].append(np.mean(self.train_cosine))\n",
    "        self.train_losses = []\n",
    "        self.train_mse = []\n",
    "        self.train_cosine = []\n",
    "        super().on_train_epoch_end()\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        self.val_history[\"val_loss\"].append(np.mean(self.val_losses))\n",
    "        self.val_history[\"val_mse\"].append(np.mean(self.val_mse))\n",
    "        self.val_history[\"val_cosine\"].append(np.mean(self.val_cosine))\n",
    "        self.val_losses = []\n",
    "        self.val_mse = []\n",
    "        self.val_cosine = []\n",
    "        super().on_validation_epoch_end()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-4, weight_decay=1e-3)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=50, verbose=True)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = 256  \n",
    "\n",
    "# Create the final dataset\n",
    "train_dataset = TensorDataset(cross_train_data, cross_train_embeds, cross_train_subjects)\n",
    "test_dataset = TensorDataset(cross_train_data, cross_test_embeds, cross_test_subjects)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BS, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BS, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name  | Type    | Params | Mode \n",
      "------------------------------------------\n",
      "0 | model | Encoder | 119 M  | train\n",
      "------------------------------------------\n",
      "119 M     Trainable params\n",
      "0         Non-trainable params\n",
      "119 M     Total params\n",
      "476.676   Total estimated model params size (MB)\n",
      "59        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1393d2d491e46f68c16da7cddf3d3e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b5ed20bc67f447b8a212147432ec760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "398fd1ed530f4136a836f51e8fab6011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db96b70201394eea97cf8a65c8108339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45e1e39dc34943aa9a008f3dd5fc779b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d55c46602614f6895285ea68d144d83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d32fcf470b3049a98b4ab6f8c14b775a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    }
   ],
   "source": [
    "brain_model = ContrastiveModel(num_input_channels=cross_train_data.shape[-1], base_channel_size=[1024], latent_dim=1280, act_fn=nn.GELU, loss_type=\"contrastive\")\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=10, devices=[2])\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(brain_model, train_dataloader, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#RIDGE REGRESION\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "train_pred_embeddings = []\n",
    "train_gt_embeddings = []\n",
    "\n",
    "brain_model.eval()\n",
    "with torch.no_grad():\n",
    "    for x, y, k in train_dataloader:\n",
    "        x = x.to(\"cpu\")\n",
    "        y = y.to(\"cpu\")\n",
    "        k = k.to(\"cpu\")\n",
    "        y_hat = brain_model(x, k=k)\n",
    "        train_pred_embeddings.append(y_hat.cpu().numpy())\n",
    "        train_gt_embeddings.append(y.cpu().numpy())\n",
    "\n",
    "train_pred_embeddings = np.vstack(train_pred_embeddings)\n",
    "train_gt_embeddings = np.vstack(train_gt_embeddings)\n",
    "\n",
    "# Train Ridge regression model\n",
    "ridge_reg = Ridge(alpha=50000.0)\n",
    "ridge_reg.fit(train_pred_embeddings, train_gt_embeddings)\n",
    "\n",
    "# Predict embeddings for the test set\n",
    "test_pred_embeddings = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y, k in test_dataloader:\n",
    "        x = x.to(\"cpu\")\n",
    "        y = y.to(\"cpu\")\n",
    "        k = k.to(\"cpu\")\n",
    "        y_hat = brain_model(x, k=k)\n",
    "        test_pred_embeddings.append(y_hat.cpu().numpy())\n",
    "\n",
    "test_pred_embeddings = np.vstack(test_pred_embeddings)\n",
    "\n",
    "# Apply Ridge regression on the test set embeddings\n",
    "refined_test_embeddings = ridge_reg.predict(test_pred_embeddings)\n",
    "\n",
    "# Convert refined embeddings to tensor\n",
    "refined_test_embeddings = torch.tensor(refined_test_embeddings).float()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOLD_stimulus_embeddings_test = BOLD_stimulus_embeddings_test.to(\"cpu\")\n",
    "\n",
    "BOLD_pred_IP = refined_test_embeddings.to(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "NSD_stimulus_embeddings_test = NSD_test_img_embeds.to(\"cpu\")\n",
    "NSD_pred_IP = refined_test_embeddings.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOD_stimulus_embeddings_test = GOD_test_image_embds.to(\"cpu\")\n",
    "GOD_pred_IP = refined_test_embeddings.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "622e66e4e7fb451c929786bc8d6b6260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from diffusers import AutoPipelineForText2Image\n",
    "from diffusers.utils import load_image\n",
    "import torch\n",
    "\n",
    "pipeline = AutoPipelineForText2Image.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16).to(\"cuda:2\")\n",
    "pipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=\"ip-adapter_sdxl.bin\")\n",
    "pipeline.set_ip_adapter_scale(1.0)\n",
    "\n",
    "#RESHAPING THE EMBEDDINGS\n",
    "\n",
    "y_pred_IP = torch.zeros((2, refined_test_embeddings.shape[0], 1, refined_test_embeddings.shape[-1]), dtype=torch.float16)\n",
    "y_pred_IP[1] = refined_test_embeddings.unsqueeze(1)\n",
    "y_pred_IP[0] = torch.zeros_like(refined_test_embeddings.unsqueeze(1))\n",
    "y_pred_IP = y_pred_IP.transpose(0,1)\n",
    "\n",
    "y_pred_IP.shape\n",
    "torch.save(y_pred_IP, \"New_results/BOLD_predicted_embeddings_08042025.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brain-decoding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
