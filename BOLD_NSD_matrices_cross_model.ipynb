{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "import torchvision.transforms as transforms\n",
    "warnings.filterwarnings('ignore')\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading PNG's and converting into tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4623 images as a tensor in /srv/nfs-data/sisko/kashif/New_results/cross_model_final.pt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "def load_images_and_save_tensor(image_folder, output_file):\n",
    "    # Get all PNG files and sort them numerically\n",
    "    image_files = sorted([f for f in os.listdir(image_folder) if f.endswith('.png')], key=lambda x: int(os.path.splitext(x)[0]))\n",
    "    \n",
    "    images = []\n",
    "    transform = transforms.ToTensor()\n",
    "    \n",
    "    for img_file in image_files:\n",
    "        img_path = os.path.join(image_folder, img_file)\n",
    "        img = Image.open(img_path).convert('RGB')  # Convert to RGB to maintain consistency\n",
    "        img_tensor = transform(img)  # Convert image to tensor\n",
    "        images.append(img_tensor)\n",
    "    \n",
    "    images_tensor = torch.stack(images)  # Stack into a single tensor\n",
    "    torch.save(images_tensor, output_file)  # Save tensor to .pt file\n",
    "    print(f\"Saved {len(images)} images as a tensor in {output_file}\")\n",
    "\n",
    "load_images_and_save_tensor(\"/New_results/cross_model_final\", \"/New_results/cross_model_final.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BOLD_test_stimulus = torch.load(\"/BOLD5000_V2/test_stimulus.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NSD_test_stimulus = torch.load(\"/decoding_NSD/data_fmri_nsd/img_test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOD_test_stimulus = torch.load(\"/decoding/GOD/test_stimulus.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_model_reconstructed = torch.load(\"/New_results/cross_model_final.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3928, 3, 1024, 1024]),\n",
       " torch.Size([445, 3, 1024, 1024]),\n",
       " torch.Size([250, 3, 1024, 1024]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NSD_recontructed_images = cross_model_reconstructed[:3928]\n",
    "BOLD_recontructed_images = cross_model_reconstructed[3928:4373]\n",
    "GOD_recontructed_images = cross_model_reconstructed[4373:]\n",
    "\n",
    "NSD_recontructed_images.shape, BOLD_recontructed_images.shape, GOD_recontructed_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torchvision.transforms as transforms\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # Load tensors (assuming they are already loaded as 'original_tensors' and 'reconstructed_tensors')\n",
    "# # original_tensors: torch.Size([3928, 3, 1024, 1024])\n",
    "# # reconstructed_tensors: torch.Size([3928, 3, 425, 425])\n",
    "\n",
    "# # Define transformation to convert tensors to PIL images\n",
    "# to_pil = transforms.ToPILImage()\n",
    "\n",
    "# # Number of images to display\n",
    "# num_images = 10\n",
    "# images_per_subject = 982\n",
    "# subject_ids = [1, 2, 5, 7]  # NSD subject IDs\n",
    "# num_subjects = len(subject_ids)\n",
    "\n",
    "# # Initialize lists for images\n",
    "# original_samples = NSD_test_stimulus[:num_images]\n",
    "# reconstructed_samples = {subject: NSD_recontructed_images[i * images_per_subject: (i * images_per_subject) + num_images] for i, subject in enumerate(subject_ids)}\n",
    "\n",
    "# # Plot the original and reconstructed images side by side\n",
    "# fig, axes = plt.subplots(num_images, num_subjects + 1, figsize=(15, num_images * 5))\n",
    "\n",
    "# for i in range(num_images):\n",
    "#     # Convert original tensor to PIL image\n",
    "#     ori_img = to_pil(original_samples[i])\n",
    "    \n",
    "#     # Plot original image in the first column\n",
    "#     axes[i, 0].imshow(ori_img)\n",
    "#     axes[i, 0].axis('off')\n",
    "#     axes[i, 0].set_title('Original Image')\n",
    "    \n",
    "#     # Plot reconstructed images for each subject\n",
    "#     for j, subject in enumerate(subject_ids):\n",
    "#         rec_img = to_pil(reconstructed_samples[subject][i])\n",
    "#         axes[i, j + 1].imshow(rec_img)\n",
    "#         axes[i, j + 1].axis('off')\n",
    "#         axes[i, j + 1].set_title(f'Subject {subject}')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import ToPILImage\n",
    "import random\n",
    "\n",
    "\n",
    "# Subject index ranges\n",
    "subject_slices = {\n",
    "    'Subject1': (0, 113),\n",
    "    'Subject2': (113, 226),\n",
    "    'Subject3': (226, 339),\n",
    "    'Subject4': (339, 445),\n",
    "}\n",
    "\n",
    "# Setup\n",
    "to_pil = ToPILImage()\n",
    "n_rows = 100\n",
    "n_cols = 8  # 4 subjects × (original + recon)\n",
    "\n",
    "\n",
    "# Sample indices for each subject\n",
    "subject_indices = {}\n",
    "for subj, (start, end) in subject_slices.items():\n",
    "    subject_indices[subj] = list(range(start, start + n_rows))\n",
    "\n",
    "# Create one big figure\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(n_cols * 2, n_rows * 2))\n",
    "\n",
    "for row in range(n_rows):\n",
    "    for col, subj in enumerate(['Subject1', 'Subject2', 'Subject3', 'Subject4']):\n",
    "        idx = subject_indices[subj][row]\n",
    "\n",
    "        orig_img = to_pil(BOLD_test_stimulus[idx].cpu())\n",
    "        recon_img = to_pil(BOLD_recontructed_images[idx].cpu())\n",
    "\n",
    "        axs[row, col * 2].imshow(orig_img)\n",
    "        axs[row, col * 2].axis('off')\n",
    "        axs[row, col * 2].set_title(f'{subj} Original', fontsize=8)\n",
    "\n",
    "        axs[row, col * 2 + 1].imshow(recon_img)\n",
    "        axs[row, col * 2 + 1].axis('off')\n",
    "        axs[row, col * 2 + 1].set_title(f'{subj} Reconstructed', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Define transformation to convert tensors to PIL images\n",
    "to_pil = transforms.ToPILImage()\n",
    "\n",
    "# Number of images to display\n",
    "num_images = 50\n",
    "images_per_subject = 50\n",
    "subject_ids = [1, 2, 3, 4, 5]\n",
    "num_subjects = len(subject_ids)\n",
    "\n",
    "# Initialize lists for images\n",
    "original_samples = GOD_test_stimulus[:num_images]\n",
    "reconstructed_samples = {subject: GOD_recontructed_images[i * images_per_subject: (i * images_per_subject) + num_images] for i, subject in enumerate(subject_ids)}\n",
    "\n",
    "# Plot the original and reconstructed images side by side\n",
    "fig, axes = plt.subplots(num_images, num_subjects + 1, figsize=(15, num_images * 5))\n",
    "\n",
    "for i in range(num_images):\n",
    "    # Convert original tensor to PIL image\n",
    "    ori_img = to_pil(original_samples[i])\n",
    "    \n",
    "    # Plot original image in the first column\n",
    "    axes[i, 0].imshow(ori_img)\n",
    "    axes[i, 0].axis('off')\n",
    "    axes[i, 0].set_title('Original Image')\n",
    "    \n",
    "    # Plot reconstructed images for each subject\n",
    "    for j, subject in enumerate(subject_ids):\n",
    "        rec_img = to_pil(reconstructed_samples[subject][i])\n",
    "        axes[i, j + 1].imshow(rec_img)\n",
    "        axes[i, j + 1].axis('off')\n",
    "        axes[i, j + 1].set_title(f'Subject {subject}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4623, 3, 425, 425])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bold_resized = F.interpolate(BOLD_test_stimulus, size=(425, 425), mode='bilinear', align_corners=False)\n",
    "GOD_resized = F.interpolate(GOD_test_stimulus, size=(425, 425), mode='bilinear', align_corners=False)\n",
    "NSD_BOLD_GOD_Combine = torch.cat((NSD_test_stimulus, bold_resized, GOD_resized), dim=0)\n",
    "NSD_BOLD_GOD_Combine.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.color import rgb2gray\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import scipy.spatial as sp\n",
    "import clip\n",
    "from PIL import Image\n",
    "from scipy.spatial.distance import correlation\n",
    "\n",
    "@torch.no_grad()\n",
    "def two_way_identification(all_brain_recons, all_images, model, preprocess, feature_layer=None, return_avg=True, device= device):\n",
    "    preds = model(torch.stack([preprocess(recon) for recon in all_brain_recons], dim=0).to(device))\n",
    "    reals = model(torch.stack([preprocess(indiv) for indiv in all_images], dim=0).to(device))\n",
    "    if feature_layer is None:\n",
    "        preds = preds.float().flatten(1).cpu().numpy()\n",
    "        reals = reals.float().flatten(1).cpu().numpy()\n",
    "    else:\n",
    "        preds = preds[feature_layer].float().flatten(1).cpu().numpy()\n",
    "        reals = reals[feature_layer].float().flatten(1).cpu().numpy()\n",
    "\n",
    "    r = np.corrcoef(reals, preds)\n",
    "    r = r[:len(all_images), len(all_images):]\n",
    "    congruents = np.diag(r)\n",
    "\n",
    "    success = r < congruents\n",
    "    success_cnt = np.sum(success, 0)\n",
    "\n",
    "    if return_avg:\n",
    "        perf = np.mean(success_cnt) / (len(all_images)-1)\n",
    "        return perf\n",
    "    else:\n",
    "        return success_cnt, len(all_images)-1\n",
    "\n",
    "def cal_metrics(all_images, all_brain_recons, device):\n",
    "    all_images = all_images[:].to(device)\n",
    "    all_brain_recons = torch.stack([img for img in all_brain_recons[:]]).to(device).to(all_images.dtype).clamp(0,1).squeeze()\n",
    "\n",
    "    print(\"Images shape:\", all_images.shape)\n",
    "    print(\"Recons shape:\", all_brain_recons.shape)\n",
    "\n",
    "    # Ensure both tensors are the same size for MSE\n",
    "    resize = transforms.Resize((all_images.size(2), all_images.size(3)), interpolation=transforms.InterpolationMode.BILINEAR)\n",
    "    all_brain_recons = resize(all_brain_recons)\n",
    "\n",
    "    print(\"Images shape after resize:\", all_images.shape)\n",
    "    print(\"Recons shape after resize:\", all_brain_recons.shape)\n",
    "\n",
    "    # Preprocess\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(425, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    ])\n",
    "\n",
    "    # Flatten images while keeping the batch dimension\n",
    "    all_images_flattened = preprocess(all_images).reshape(len(all_images), -1).cpu()\n",
    "    all_brain_recons_flattened = preprocess(all_brain_recons).view(len(all_brain_recons), -1).cpu()\n",
    "\n",
    "    print(all_images_flattened.shape)\n",
    "    print(all_brain_recons_flattened.shape)\n",
    "\n",
    "    # PixCorr\n",
    "    print(\"\\n------calculating pixcorr------\")\n",
    "    corrsum = 0\n",
    "    for i in tqdm(range(len(all_images))):\n",
    "        corrsum += np.corrcoef(all_images_flattened[i], all_brain_recons_flattened[i])[0][1]\n",
    "    pixcorr = corrsum / len(all_images)\n",
    "    print(\"PixCorr:\", pixcorr)\n",
    "\n",
    "    # SSIM\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(625, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    ])\n",
    "\n",
    "    img_gray = rgb2gray(preprocess(all_images).permute((0,2,3,1)).cpu().numpy())\n",
    "    recon_gray = rgb2gray(preprocess(all_brain_recons).permute((0,2,3,1)).cpu().numpy())\n",
    "    print(\"converted, now calculating ssim...\")\n",
    "\n",
    "    ssim_score=[]\n",
    "    for im, rec in tqdm(zip(img_gray, recon_gray), total=len(all_images)):\n",
    "        ssim_score.append(ssim(rec, im, multichannel=True, gaussian_weights=True, sigma=1.5, use_sample_covariance=False, data_range=1.0))\n",
    "\n",
    "    ssim_mean = np.mean(ssim_score)\n",
    "    print(\"SSIM:\", ssim_mean)\n",
    "\n",
    "    # MSE\n",
    "    mse = torch.nn.functional.mse_loss(all_brain_recons, all_images).item()\n",
    "    print(\"MSE:\", mse)\n",
    "\n",
    "    # Cosine Similarity\n",
    "    cosine_sim = torch.nn.functional.cosine_similarity(all_brain_recons_flattened, all_images_flattened).mean().item()\n",
    "    print(\"Cosine Similarity:\", cosine_sim)\n",
    "\n",
    "    # Feature-based evaluations using different models\n",
    "    def evaluate_model(model, preprocess, feature_layers, layer_names):\n",
    "        results = {}\n",
    "        for feature_layer, layer_name in zip(feature_layers, layer_names):\n",
    "            print(f\"\\n---{layer_name}---\")\n",
    "            all_per_correct = two_way_identification(all_brain_recons.to(device).float(), all_images, \n",
    "                                                     model, preprocess, feature_layer, device=device)\n",
    "            results[layer_name] = np.mean(all_per_correct)\n",
    "            print(f\"2-way Percent Correct: {results[layer_name]:.4f}\")\n",
    "        return results\n",
    "\n",
    "    # AlexNet\n",
    "    from torchvision.models import alexnet, AlexNet_Weights\n",
    "    alex_weights = AlexNet_Weights.IMAGENET1K_V1\n",
    "    alex_model = create_feature_extractor(alexnet(weights=alex_weights), return_nodes=['features.4', 'features.11']).to(device)\n",
    "    alex_model.eval().requires_grad_(False)\n",
    "\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    alexnet_results = evaluate_model(alex_model, preprocess, ['features.4', 'features.11'], ['AlexNet(2)', 'AlexNet(5)'])\n",
    "    del alex_model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # InceptionV3\n",
    "    from torchvision.models import inception_v3, Inception_V3_Weights\n",
    "    inception_weights = Inception_V3_Weights.DEFAULT\n",
    "    inception_model = create_feature_extractor(inception_v3(weights=inception_weights), return_nodes=['avgpool']).to(device)\n",
    "    inception_model.eval().requires_grad_(False)\n",
    "\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(342, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    inception_results = evaluate_model(inception_model, preprocess, ['avgpool'], ['InceptionV3'])\n",
    "    del inception_model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    #CLIP\n",
    "    clip_model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(224, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                            std=[0.26862954, 0.26130258, 0.27577711]),\n",
    "    ])\n",
    "\n",
    "    all_per_correct = two_way_identification(all_brain_recons, all_images,\n",
    "                                            clip_model.encode_image, preprocess, None) # final layer\n",
    "    clip_results = np.mean(all_per_correct)\n",
    "    print(\"CLIP:\", clip_results)\n",
    "\n",
    "    # EfficientNet\n",
    "    from torchvision.models import efficientnet_b1, EfficientNet_B1_Weights\n",
    "    eff_weights = EfficientNet_B1_Weights.DEFAULT\n",
    "    eff_model = create_feature_extractor(efficientnet_b1(weights=eff_weights), return_nodes=['avgpool']).to(device)\n",
    "    eff_model.eval().requires_grad_(False)\n",
    "\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(255, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    gt = eff_model(preprocess(all_images))['avgpool']\n",
    "    gt = gt.reshape(len(gt), -1).cpu().numpy()\n",
    "    fake = eff_model(preprocess(all_brain_recons))['avgpool']\n",
    "    fake = fake.reshape(len(fake), -1).cpu().numpy()\n",
    "    effnet_distance = np.array([sp.distance.correlation(gt[i], fake[i]) for i in range(len(gt))]).mean()\n",
    "    print(\"EffNet Distance:\", effnet_distance)\n",
    "    del eff_model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # SwAV\n",
    "    swav_model = torch.hub.load('facebookresearch/swav:main', 'resnet50')\n",
    "    swav_model = create_feature_extractor(swav_model, return_nodes=['avgpool']).to(device)\n",
    "    swav_model.eval().requires_grad_(False)\n",
    "\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(224, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    gt = swav_model(preprocess(all_images))['avgpool']\n",
    "    gt = gt.reshape(len(gt), -1).cpu().numpy()\n",
    "    fake = swav_model(preprocess(all_brain_recons))['avgpool']\n",
    "    fake = fake.reshape(len(fake), -1).cpu().numpy()\n",
    "    swav_distance = np.array([correlation(gt[i], fake[i]) for i in range(len(gt))]).mean()\n",
    "    print(\"SwAV Distance:\", swav_distance)\n",
    "    del swav_model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Save the results\n",
    "    metrics = {\n",
    "        'PixCorr': [pixcorr],\n",
    "        'SSIM': [ssim_mean],\n",
    "        'MSE': [mse],\n",
    "        'Cosine Similarity': [cosine_sim],\n",
    "        'AlexNet(2)': [alexnet_results[\"AlexNet(2)\"]],\n",
    "        'AlexNet(5)': [alexnet_results[\"AlexNet(5)\"]],\n",
    "        'InceptionV3': [inception_results[\"InceptionV3\"]],\n",
    "        'CLIP': [clip_results],  # corrected line\n",
    "        'EffNet Distance': [effnet_distance],\n",
    "        'SwAV Distance': [swav_distance]\n",
    "    }\n",
    "    return metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing subject1...\n",
      "Images shape: torch.Size([982, 3, 425, 425])\n",
      "Recons shape: torch.Size([982, 3, 1024, 1024])\n",
      "Images shape after resize: torch.Size([982, 3, 425, 425])\n",
      "Recons shape after resize: torch.Size([982, 3, 425, 425])\n",
      "torch.Size([982, 541875])\n",
      "torch.Size([982, 541875])\n",
      "\n",
      "------calculating pixcorr------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 982/982 [00:01<00:00, 640.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PixCorr: 0.06303688303255169\n",
      "converted, now calculating ssim...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 982/982 [00:17<00:00, 57.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSIM: 0.3376463062255907\n",
      "MSE: 0.1145549938082695\n",
      "Cosine Similarity: 0.7825461626052856\n",
      "\n",
      "---AlexNet(2)---\n",
      "2-way Percent Correct: 0.8063\n",
      "\n",
      "---AlexNet(5)---\n",
      "2-way Percent Correct: 0.9190\n",
      "\n",
      "---InceptionV3---\n",
      "2-way Percent Correct: 0.9265\n",
      "CLIP: 0.9414205962160894\n",
      "EffNet Distance: 0.6895740615917768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/kashif/.cache/torch/hub/facebookresearch_swav_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwAV Distance: 0.3866674695156197\n",
      "\n",
      "Processing subject2...\n",
      "Images shape: torch.Size([982, 3, 425, 425])\n",
      "Recons shape: torch.Size([982, 3, 1024, 1024])\n",
      "Images shape after resize: torch.Size([982, 3, 425, 425])\n",
      "Recons shape after resize: torch.Size([982, 3, 425, 425])\n",
      "torch.Size([982, 541875])\n",
      "torch.Size([982, 541875])\n",
      "\n",
      "------calculating pixcorr------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 982/982 [00:01<00:00, 750.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PixCorr: 0.06404633235039306\n",
      "converted, now calculating ssim...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 982/982 [00:16<00:00, 57.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSIM: 0.3379220873522155\n",
      "MSE: 0.11432468146085739\n",
      "Cosine Similarity: 0.781416118144989\n",
      "\n",
      "---AlexNet(2)---\n",
      "2-way Percent Correct: 0.8100\n",
      "\n",
      "---AlexNet(5)---\n",
      "2-way Percent Correct: 0.9232\n",
      "\n",
      "---InceptionV3---\n",
      "2-way Percent Correct: 0.9266\n",
      "CLIP: 0.9319514772531459\n",
      "EffNet Distance: 0.7050631283804736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/kashif/.cache/torch/hub/facebookresearch_swav_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwAV Distance: 0.396642854508883\n",
      "\n",
      "Processing subject5...\n",
      "Images shape: torch.Size([982, 3, 425, 425])\n",
      "Recons shape: torch.Size([982, 3, 1024, 1024])\n",
      "Images shape after resize: torch.Size([982, 3, 425, 425])\n",
      "Recons shape after resize: torch.Size([982, 3, 425, 425])\n",
      "torch.Size([982, 541875])\n",
      "torch.Size([982, 541875])\n",
      "\n",
      "------calculating pixcorr------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 982/982 [00:01<00:00, 802.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PixCorr: 0.06496410980078911\n",
      "converted, now calculating ssim...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 982/982 [00:16<00:00, 59.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSIM: 0.33996850282505714\n",
      "MSE: 0.11399119347333908\n",
      "Cosine Similarity: 0.7817409634590149\n",
      "\n",
      "---AlexNet(2)---\n",
      "2-way Percent Correct: 0.8102\n",
      "\n",
      "---AlexNet(5)---\n",
      "2-way Percent Correct: 0.9190\n",
      "\n",
      "---InceptionV3---\n",
      "2-way Percent Correct: 0.9262\n",
      "CLIP: 0.9440406418488968\n",
      "EffNet Distance: 0.685955386859804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/kashif/.cache/torch/hub/facebookresearch_swav_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwAV Distance: 0.383147000366125\n",
      "\n",
      "Processing subject7...\n",
      "Images shape: torch.Size([982, 3, 425, 425])\n",
      "Recons shape: torch.Size([982, 3, 1024, 1024])\n",
      "Images shape after resize: torch.Size([982, 3, 425, 425])\n",
      "Recons shape after resize: torch.Size([982, 3, 425, 425])\n",
      "torch.Size([982, 541875])\n",
      "torch.Size([982, 541875])\n",
      "\n",
      "------calculating pixcorr------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 982/982 [00:01<00:00, 799.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PixCorr: 0.0676461679428338\n",
      "converted, now calculating ssim...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 982/982 [00:15<00:00, 61.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSIM: 0.33514182883399135\n",
      "MSE: 0.11407474428415298\n",
      "Cosine Similarity: 0.7857064008712769\n",
      "\n",
      "---AlexNet(2)---\n",
      "2-way Percent Correct: 0.7868\n",
      "\n",
      "---AlexNet(5)---\n",
      "2-way Percent Correct: 0.9001\n",
      "\n",
      "---InceptionV3---\n",
      "2-way Percent Correct: 0.9092\n",
      "CLIP: 0.9249570765107303\n",
      "EffNet Distance: 0.7136681880021835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/kashif/.cache/torch/hub/facebookresearch_swav_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwAV Distance: 0.4049867496376099\n",
      "\n",
      "Processing subject8...\n",
      "Images shape: torch.Size([113, 3, 425, 425])\n",
      "Recons shape: torch.Size([113, 3, 1024, 1024])\n",
      "Images shape after resize: torch.Size([113, 3, 425, 425])\n",
      "Recons shape after resize: torch.Size([113, 3, 425, 425])\n",
      "torch.Size([113, 541875])\n",
      "torch.Size([113, 541875])\n",
      "\n",
      "------calculating pixcorr------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113/113 [00:00<00:00, 736.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PixCorr: 0.033629428999447895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted, now calculating ssim...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113/113 [00:01<00:00, 61.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSIM: 0.3787125321777612\n",
      "MSE: 0.11293075233697891\n",
      "Cosine Similarity: 0.777799665927887\n",
      "\n",
      "---AlexNet(2)---\n",
      "2-way Percent Correct: 0.7069\n",
      "\n",
      "---AlexNet(5)---\n",
      "2-way Percent Correct: 0.8237\n",
      "\n",
      "---InceptionV3---\n",
      "2-way Percent Correct: 0.7062\n",
      "CLIP: 0.7664348925410873\n",
      "EffNet Distance: 0.8946479359553924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/kashif/.cache/torch/hub/facebookresearch_swav_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwAV Distance: 0.5420178037631832\n",
      "\n",
      "Processing subject9...\n",
      "Images shape: torch.Size([113, 3, 425, 425])\n",
      "Recons shape: torch.Size([113, 3, 1024, 1024])\n",
      "Images shape after resize: torch.Size([113, 3, 425, 425])\n",
      "Recons shape after resize: torch.Size([113, 3, 425, 425])\n",
      "torch.Size([113, 541875])\n",
      "torch.Size([113, 541875])\n",
      "\n",
      "------calculating pixcorr------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113/113 [00:00<00:00, 771.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PixCorr: 0.019490338119794646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted, now calculating ssim...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113/113 [00:01<00:00, 60.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSIM: 0.3730833337385661\n",
      "MSE: 0.11912981420755386\n",
      "Cosine Similarity: 0.7724331021308899\n",
      "\n",
      "---AlexNet(2)---\n",
      "2-way Percent Correct: 0.6757\n",
      "\n",
      "---AlexNet(5)---\n",
      "2-way Percent Correct: 0.7485\n",
      "\n",
      "---InceptionV3---\n",
      "2-way Percent Correct: 0.6609\n",
      "CLIP: 0.714680783817952\n",
      "EffNet Distance: 0.9245341491973111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/kashif/.cache/torch/hub/facebookresearch_swav_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwAV Distance: 0.5778855604514083\n",
      "\n",
      "Processing subject10...\n",
      "Images shape: torch.Size([113, 3, 425, 425])\n",
      "Recons shape: torch.Size([113, 3, 1024, 1024])\n",
      "Images shape after resize: torch.Size([113, 3, 425, 425])\n",
      "Recons shape after resize: torch.Size([113, 3, 425, 425])\n",
      "torch.Size([113, 541875])\n",
      "torch.Size([113, 541875])\n",
      "\n",
      "------calculating pixcorr------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113/113 [00:00<00:00, 740.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PixCorr: 0.042105902268694095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted, now calculating ssim...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113/113 [00:01<00:00, 61.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSIM: 0.37561151445801216\n",
      "MSE: 0.11604425311088562\n",
      "Cosine Similarity: 0.7753045558929443\n",
      "\n",
      "---AlexNet(2)---\n",
      "2-way Percent Correct: 0.6448\n",
      "\n",
      "---AlexNet(5)---\n",
      "2-way Percent Correct: 0.7714\n",
      "\n",
      "---InceptionV3---\n",
      "2-way Percent Correct: 0.7173\n",
      "CLIP: 0.7568742098609356\n",
      "EffNet Distance: 0.9150810840843947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/kashif/.cache/torch/hub/facebookresearch_swav_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwAV Distance: 0.5629533629704593\n",
      "\n",
      "Processing subject11...\n",
      "Images shape: torch.Size([106, 3, 425, 425])\n",
      "Recons shape: torch.Size([106, 3, 1024, 1024])\n",
      "Images shape after resize: torch.Size([106, 3, 425, 425])\n",
      "Recons shape after resize: torch.Size([106, 3, 425, 425])\n",
      "torch.Size([106, 541875])\n",
      "torch.Size([106, 541875])\n",
      "\n",
      "------calculating pixcorr------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:00<00:00, 731.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PixCorr: 0.04384213116365503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted, now calculating ssim...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:01<00:00, 60.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSIM: 0.37497458333140726\n",
      "MSE: 0.1149488165974617\n",
      "Cosine Similarity: 0.7808213829994202\n",
      "\n",
      "---AlexNet(2)---\n",
      "2-way Percent Correct: 0.7143\n",
      "\n",
      "---AlexNet(5)---\n",
      "2-way Percent Correct: 0.8188\n",
      "\n",
      "---InceptionV3---\n",
      "2-way Percent Correct: 0.7130\n",
      "CLIP: 0.780952380952381\n",
      "EffNet Distance: 0.9114474792701386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/kashif/.cache/torch/hub/facebookresearch_swav_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwAV Distance: 0.5452974103243928\n",
      "\n",
      "Processing subject12...\n",
      "Images shape: torch.Size([50, 3, 425, 425])\n",
      "Recons shape: torch.Size([50, 3, 1024, 1024])\n",
      "Images shape after resize: torch.Size([50, 3, 425, 425])\n",
      "Recons shape after resize: torch.Size([50, 3, 425, 425])\n",
      "torch.Size([50, 541875])\n",
      "torch.Size([50, 541875])\n",
      "\n",
      "------calculating pixcorr------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 810.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PixCorr: 0.05199317191856506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted, now calculating ssim...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 60.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSIM: 0.3411259384904603\n",
      "MSE: 0.12853121757507324\n",
      "Cosine Similarity: 0.7547575235366821\n",
      "\n",
      "---AlexNet(2)---\n",
      "2-way Percent Correct: 0.7110\n",
      "\n",
      "---AlexNet(5)---\n",
      "2-way Percent Correct: 0.8412\n",
      "\n",
      "---InceptionV3---\n",
      "2-way Percent Correct: 0.7151\n",
      "CLIP: 0.7848979591836734\n",
      "EffNet Distance: 0.9248186806504047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/kashif/.cache/torch/hub/facebookresearch_swav_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwAV Distance: 0.57542197040215\n",
      "\n",
      "Processing subject13...\n",
      "Images shape: torch.Size([50, 3, 425, 425])\n",
      "Recons shape: torch.Size([50, 3, 1024, 1024])\n",
      "Images shape after resize: torch.Size([50, 3, 425, 425])\n",
      "Recons shape after resize: torch.Size([50, 3, 425, 425])\n",
      "torch.Size([50, 541875])\n",
      "torch.Size([50, 541875])\n",
      "\n",
      "------calculating pixcorr------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 729.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PixCorr: 0.006128569235353803\n",
      "converted, now calculating ssim...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 59.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSIM: 0.32120155601480893\n",
      "MSE: 0.1338493525981903\n",
      "Cosine Similarity: 0.7377384305000305\n",
      "\n",
      "---AlexNet(2)---\n",
      "2-way Percent Correct: 0.6722\n",
      "\n",
      "---AlexNet(5)---\n",
      "2-way Percent Correct: 0.8139\n",
      "\n",
      "---InceptionV3---\n",
      "2-way Percent Correct: 0.6731\n",
      "CLIP: 0.8453061224489796\n",
      "EffNet Distance: 0.9141452738170132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/kashif/.cache/torch/hub/facebookresearch_swav_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwAV Distance: 0.5634931046845765\n",
      "\n",
      "Processing subject14...\n",
      "Images shape: torch.Size([50, 3, 425, 425])\n",
      "Recons shape: torch.Size([50, 3, 1024, 1024])\n",
      "Images shape after resize: torch.Size([50, 3, 425, 425])\n",
      "Recons shape after resize: torch.Size([50, 3, 425, 425])\n",
      "torch.Size([50, 541875])\n",
      "torch.Size([50, 541875])\n",
      "\n",
      "------calculating pixcorr------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 742.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PixCorr: 0.03363652577853462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted, now calculating ssim...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 59.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSIM: 0.3318005115071707\n",
      "MSE: 0.12873952090740204\n",
      "Cosine Similarity: 0.7382766008377075\n",
      "\n",
      "---AlexNet(2)---\n",
      "2-way Percent Correct: 0.7437\n",
      "\n",
      "---AlexNet(5)---\n",
      "2-way Percent Correct: 0.8690\n",
      "\n",
      "---InceptionV3---\n",
      "2-way Percent Correct: 0.6106\n",
      "CLIP: 0.8155102040816327\n",
      "EffNet Distance: 0.9223298053519089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/kashif/.cache/torch/hub/facebookresearch_swav_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwAV Distance: 0.553029740301233\n",
      "\n",
      "Processing subject15...\n",
      "Images shape: torch.Size([50, 3, 425, 425])\n",
      "Recons shape: torch.Size([50, 3, 1024, 1024])\n",
      "Images shape after resize: torch.Size([50, 3, 425, 425])\n",
      "Recons shape after resize: torch.Size([50, 3, 425, 425])\n",
      "torch.Size([50, 541875])\n",
      "torch.Size([50, 541875])\n",
      "\n",
      "------calculating pixcorr------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 784.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PixCorr: 0.034170926154237384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted, now calculating ssim...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 60.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSIM: 0.33428428201347704\n",
      "MSE: 0.13001148402690887\n",
      "Cosine Similarity: 0.7602849006652832\n",
      "\n",
      "---AlexNet(2)---\n",
      "2-way Percent Correct: 0.7298\n",
      "\n",
      "---AlexNet(5)---\n",
      "2-way Percent Correct: 0.8580\n",
      "\n",
      "---InceptionV3---\n",
      "2-way Percent Correct: 0.7143\n",
      "CLIP: 0.8420408163265306\n",
      "EffNet Distance: 0.8962686529077939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/kashif/.cache/torch/hub/facebookresearch_swav_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwAV Distance: 0.5555986318587284\n",
      "\n",
      "Processing subject16...\n",
      "Images shape: torch.Size([50, 3, 425, 425])\n",
      "Recons shape: torch.Size([50, 3, 1024, 1024])\n",
      "Images shape after resize: torch.Size([50, 3, 425, 425])\n",
      "Recons shape after resize: torch.Size([50, 3, 425, 425])\n",
      "torch.Size([50, 541875])\n",
      "torch.Size([50, 541875])\n",
      "\n",
      "------calculating pixcorr------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 812.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PixCorr: 0.017489945389707946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted, now calculating ssim...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 60.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSIM: 0.3566275423336576\n",
      "MSE: 0.12199704349040985\n",
      "Cosine Similarity: 0.7541359066963196\n",
      "\n",
      "---AlexNet(2)---\n",
      "2-way Percent Correct: 0.7629\n",
      "\n",
      "---AlexNet(5)---\n",
      "2-way Percent Correct: 0.8661\n",
      "\n",
      "---InceptionV3---\n",
      "2-way Percent Correct: 0.7065\n",
      "CLIP: 0.7644897959183674\n",
      "EffNet Distance: 0.9077663490629392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/kashif/.cache/torch/hub/facebookresearch_swav_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwAV Distance: 0.5591891438549023\n",
      "\n",
      "Average Metrics Across Subjects:\n",
      "PixCorr: 0.0417\n",
      "SSIM: 0.3491\n",
      "MSE: 0.1202\n",
      "Cosine Similarity: 0.7679\n",
      "AlexNet(2): 0.7365\n",
      "AlexNet(5): 0.8517\n",
      "InceptionV3: 0.7620\n",
      "CLIP: 0.8318\n",
      "EffNet Distance: 0.8466\n",
      "SwAV Distance: 0.5082\n"
     ]
    }
   ],
   "source": [
    "\n",
    "subject_indices = {\n",
    "    \"subject1\": slice(0, 982),\n",
    "    \"subject2\": slice(982, 1964),\n",
    "    \"subject5\": slice(1964, 2946),\n",
    "    \"subject7\": slice(2946, 3928),\n",
    "    \"subject8\": slice(3928, 4041),\n",
    "    \"subject9\": slice(4041, 4154),\n",
    "    \"subject10\": slice(4154, 4267),\n",
    "    \"subject11\": slice(4267, 4373),\n",
    "    \"subject12\": slice(4373, 4423),\n",
    "    \"subject13\": slice(4423, 4473),\n",
    "    \"subject14\": slice(4473, 4523),\n",
    "    \"subject15\": slice(4523, 4573),\n",
    "    \"subject16\": slice(4573, 4623),\n",
    "}\n",
    "\n",
    "def calculate_subject_wise_metrics(all_images, all_brain_recons, device):\n",
    "    subject_results = {}\n",
    "    \n",
    "    for subject, indices in subject_indices.items():\n",
    "        print(f\"\\nProcessing {subject}...\")\n",
    "        subject_images = all_images[indices]\n",
    "        subject_recons = all_brain_recons[indices]\n",
    "        \n",
    "        metrics = cal_metrics(subject_images, subject_recons, device)\n",
    "        subject_results[subject] = metrics\n",
    "    \n",
    "\n",
    "    avg_metrics = {}\n",
    "    for key in subject_results[\"subject1\"].keys(): \n",
    "        avg_metrics[key] = np.mean([subject_results[subj][key] for subj in subject_results])\n",
    "    \n",
    "    print(\"\\nAverage Metrics Across Subjects:\")\n",
    "    for key, value in avg_metrics.items():\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "    \n",
    "    # Save results\n",
    "    df = pd.DataFrame.from_dict(subject_results, orient='index')\n",
    "    df.loc['Average'] = avg_metrics\n",
    "    df.to_csv('New_results/cross_model_final.csv', sep='\\t')\n",
    "\n",
    "# Example usage\n",
    "calculate_subject_wise_metrics(NSD_BOLD_GOD_Combine, cross_model_reconstructed, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(425, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "])\n",
    "\n",
    "BOLD_test_stimulus = preprocess(NSD_test_stimulus)\n",
    "BOLD_recontructed_images = preprocess(NSD_recontructed_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.functional import accuracy\n",
    "\n",
    "def n_way_top_k_acc(pred, class_id, n_way, num_trials=40, top_k=1):\n",
    "    pick_range =[i for i in np.arange(len(pred)) if i != class_id]\n",
    "    acc_list = []\n",
    "    for t in range(num_trials):\n",
    "        idxs_picked = np.random.choice(pick_range, n_way-1, replace=False)\n",
    "        pred_picked = torch.cat([pred[class_id].unsqueeze(0), pred[idxs_picked]])\n",
    "        acc = accuracy(pred_picked.unsqueeze(0), torch.tensor([0], device=pred.device), \n",
    "                    task='multiclass',num_classes=n_way,top_k=top_k)\n",
    "        acc_list.append(acc.item())\n",
    "    return np.mean(acc_list), np.std(acc_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import ViT_H_14_Weights, vit_h_14\n",
    "from tqdm import tqdm\n",
    "\n",
    "weights = ViT_H_14_Weights.DEFAULT\n",
    "vit_model = vit_h_14(weights=weights)\n",
    "vit_model.to(device)\n",
    "vit_model.eval()\n",
    "preprocess = weights.transforms()\n",
    "acc_list = []\n",
    "std_list = []\n",
    "\n",
    "for i in tqdm(range(len(BOLD_recontructed_images)), desc=\"Processing images\"):\n",
    "    image, recon_image = preprocess(BOLD_test_stimulus[i].unsqueeze(0)).to(device), preprocess(BOLD_recontructed_images[i].unsqueeze(0)).to(device)\n",
    "    recon_image_out = vit_model(recon_image).squeeze(0).softmax(0).detach()\n",
    "    gt_class_id = vit_model(image).squeeze(0).softmax(0).argmax().item()\n",
    "    acc, std = n_way_top_k_acc(recon_image_out, gt_class_id, 50, 1000, 1)\n",
    "    acc_list.append(acc)\n",
    "    std_list.append(std)\n",
    "\n",
    "print(\"mean acc: {}, std acc: {}\".format(np.mean(acc_list), np.std(acc_list)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
