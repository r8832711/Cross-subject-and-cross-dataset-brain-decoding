{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5176cb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "import mne\n",
    "from mne_bids import (\n",
    "    BIDSPath,\n",
    "    read_raw_bids,\n",
    "    print_dir_tree,\n",
    "    make_report,\n",
    "    find_matching_paths,\n",
    "    get_entity_vals,\n",
    ")\n",
    "\n",
    "import h5py\n",
    "from os.path import join as opj\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import tqdm\n",
    "#from versatile_diffusion_dual_guided_fake_images import *\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from os.path import join as opj\n",
    "from PIL import Image\n",
    "import h5py\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n",
    "\n",
    "#import labelencoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#import pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "import tqdm\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fde889",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NSDDataset(Dataset):\n",
    "    \n",
    "\n",
    "    \n",
    "    def __init__(self, fmri_data,imgs_data,caption_data,transforms=None):\n",
    "        self.fmri_data=np.load(fmri_data)\n",
    "        self.imgs_data=np.load(imgs_data).astype(np.uint8)\n",
    "        self.caption_data=np.load(caption_data,allow_pickle=True)\n",
    "        self.transforms=transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return  len(self.fmri_data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        fmri=torch.tensor(self.fmri_data[idx])\n",
    "        img=Image.fromarray(self.imgs_data[idx])\n",
    "        \n",
    "        if self.transforms:\n",
    "            img=self.transforms(img)\n",
    "        \n",
    "        caption=self.caption_data[idx][0] #cambiare se ne voglio altre\n",
    "        \n",
    "        return fmri,img,caption\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30d6535",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoPipelineForImage2Image\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "pipeline = AutoPipelineForImage2Image.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16).to(\"cuda:3\")\n",
    "pipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=\"ip-adapter_sdxl.bin\")\n",
    "pipeline.set_ip_adapter_scale(0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe9023a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(sub):\n",
    "    base_path=\"/storage/fMRI_NSD/data\"\n",
    "    timeseries_path=opj(base_path,\"nsddata_timeseries\")\n",
    "    betas_path=opj(base_path,\"nsddata_betas\")\n",
    "\n",
    "    stimuli_path=opj(base_path,\"nsddata_stimuli\",\"stimuli\",\"nsd\")\n",
    "    stim_file_path=opj(stimuli_path,\"nsd_stimuli.hdf5\")\n",
    "    mod=\"func1pt8mm\"\n",
    "    subj_data_path=opj(timeseries_path,\"ppdata\",sub,mod,\"timeseries\")\n",
    "    subj_betas_path=opj(betas_path,\"ppdata\",sub,mod,\"betas_assumehrf\")\n",
    "\n",
    "    subj_betas_roi_extracted_path=opj(base_path,\"processed_roi\",sub,mod)\n",
    "\n",
    "    stim_order_path=opj(base_path,\"nsddata\",\"experiments\",\"nsd\",\"nsd_expdesign.mat\")\n",
    "    stim_info_path=opj(base_path,\"nsddata\",\"experiments\",\"nsd\",\"nsd_stim_info_merged.csv\")\n",
    "    stim_captions_train_path=opj(base_path,\"nsddata_stimuli\",\"stimuli\",\"nsd\",\"annotations\",f\"captions_train2017.json\")\n",
    "    stim_captions_val_path=opj(base_path,\"nsddata_stimuli\",\"stimuli\",\"nsd\",\"annotations\",f\"captions_val2017.json\")\n",
    "\n",
    "    processed_data=opj(base_path,\"processed_data\",sub)\n",
    "\n",
    "    sub_idx=int(sub.split(\"0\")[-1])\n",
    "\n",
    "    fmri_train_data=opj(processed_data,f\"nsd_train_fmriavg_nsdgeneral_sub{sub_idx}.npy\")\n",
    "    imgs_train_data=opj(processed_data,f\"nsd_train_stim_sub{sub_idx}.npy\")\n",
    "    captions_train_data=opj(processed_data, f\"nsd_train_cap_sub{sub_idx}.npy\")\n",
    "\n",
    "    fmri_test_data=opj(processed_data,f\"nsd_test_fmriavg_nsdgeneral_sub{sub_idx}.npy\")\n",
    "    imgs_test_data=opj(processed_data,f\"nsd_test_stim_sub{sub_idx}.npy\")\n",
    "    captions_test_data=opj(processed_data, f\"nsd_test_cap_sub{sub_idx}.npy\")\n",
    "\n",
    "\n",
    "\n",
    "    tr=torchvision.transforms.ToTensor()\n",
    "    train_dataset=NSDDataset(fmri_train_data,imgs_train_data,captions_train_data,transforms=tr)\n",
    "    test_dataset=NSDDataset(fmri_test_data,imgs_test_data,captions_test_data,transforms=tr)\n",
    "\n",
    "\n",
    "    BS=32\n",
    "\n",
    "    train_dataloader=DataLoader(train_dataset,BS,shuffle=False)\n",
    "    # val_dataloader=DataLoader(val_dataset,BS,shuffle=True)\n",
    "    test_dataloader=DataLoader(test_dataset,BS,shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "    to_pil= torchvision.transforms.ToPILImage()\n",
    "\n",
    "    train_clip_img_embeds=[]\n",
    "    train_fmri_data=[]\n",
    "    train_imgs=[]\n",
    "    with torch.no_grad():\n",
    "        for x, y, c in tqdm.tqdm(train_dataloader, position=0):\n",
    "            # Save img data\n",
    "            images = [to_pil(i) for i in y]\n",
    "            # Process images one by one\n",
    "            image_features_batch = []\n",
    "            for image in images:\n",
    "                image_features = pipeline.prepare_ip_adapter_image_embeds(\n",
    "                    ip_adapter_image=image,\n",
    "                    ip_adapter_image_embeds=None,\n",
    "                    device=\"cuda:3\",\n",
    "                    num_images_per_prompt=1,\n",
    "                    do_classifier_free_guidance=True,\n",
    "                )\n",
    "                image_features_batch.append(image_features[0])  # Append the tensor directly\n",
    "            image_features_batch = torch.stack(image_features_batch, axis=0)  # Use stack instead of cat\n",
    "            \n",
    "            train_clip_img_embeds.append(image_features_batch)\n",
    "            train_fmri_data.append(x)\n",
    "            train_imgs.append(y)\n",
    "\n",
    "        train_clip_img_embeds = torch.cat(train_clip_img_embeds, axis=0)\n",
    "        train_fmri_data = torch.cat(train_fmri_data, 0)\n",
    "        train_imgs = torch.cat(train_imgs, 0)\n",
    "\n",
    "    test_clip_img_embeds = []\n",
    "    test_fmri_data = []\n",
    "    test_imgs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y, c in tqdm.tqdm(test_dataloader, position=0):\n",
    "            # Save img data\n",
    "            images = [to_pil(i) for i in y]\n",
    "            # Process images one by one\n",
    "            image_features_batch = []\n",
    "            for image in images:\n",
    "                image_features = pipeline.prepare_ip_adapter_image_embeds(\n",
    "                    ip_adapter_image=image,\n",
    "                    ip_adapter_image_embeds=None,\n",
    "                    device=\"cuda:3\",\n",
    "                    num_images_per_prompt=1,\n",
    "                    do_classifier_free_guidance=True,\n",
    "                )\n",
    "                image_features_batch.append(image_features[0])  # Append the tensor directly\n",
    "            image_features_batch = torch.stack(image_features_batch, axis=0)  # Use stack instead of cat\n",
    "            \n",
    "            test_clip_img_embeds.append(image_features_batch)\n",
    "            test_fmri_data.append(x)\n",
    "            test_imgs.append(y)\n",
    "\n",
    "        test_clip_img_embeds = torch.cat(test_clip_img_embeds, axis=0)\n",
    "        test_fmri_data = torch.cat(test_fmri_data, 0)\n",
    "        test_imgs = torch.cat(test_imgs, 0)\n",
    "\n",
    "    # Standardize data\n",
    "    mean = train_fmri_data.mean(0)\n",
    "    std = train_fmri_data.std(0)\n",
    "    train_fmri_data = (train_fmri_data - mean) / std\n",
    "    test_fmri_data = (test_fmri_data - mean) / std\n",
    "\n",
    "    test_fmri_data = torch.nan_to_num(test_fmri_data)\n",
    "    train_fmri_data = torch.nan_to_num(train_fmri_data)\n",
    "\n",
    "    return train_fmri_data, test_fmri_data, train_imgs, test_imgs, train_clip_img_embeds, test_clip_img_embeds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d950cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datas=[]\n",
    "test_datas=[]\n",
    "img_train=[]\n",
    "img_test=[]\n",
    "train_clip_img_embeds=[]\n",
    "test_clip_img_embeds=[]\n",
    "subject_train_ids=[]\n",
    "subject_test_ids=[]\n",
    "\n",
    "for p in tqdm.tqdm([\"subj01\",\"subj02\",\"subj05\",\"subj07\"]):\n",
    "    train_data_,test_data_, img_train_, img_test_,train_clip_img_embeds_, test_clip_img_embeds_=get_dataset(p)\n",
    "    img_train.append(img_train_)\n",
    "    img_test.append(img_test_)\n",
    "    train_datas.append(train_data_)\n",
    "    test_datas.append(test_data_)\n",
    "    train_clip_img_embeds.append(train_clip_img_embeds_)\n",
    "    test_clip_img_embeds.append(test_clip_img_embeds_)\n",
    "\n",
    "    subject_train_ids+=[p]*len(train_data_)\n",
    "    subject_test_ids+=[p]*len(test_data_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06099fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train data shape\",train_datas[0].shape)\n",
    "print(\"test data shape\",test_datas[0].shape)\n",
    "print(\"img shape\",img_train[0].shape)\n",
    "print(\"imgtest shape\",img_test[0].shape)\n",
    "print(\"train_emb shape\",train_clip_img_embeds[0].shape)\n",
    "print(\"test emb shape\",test_clip_img_embeds[0].shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
