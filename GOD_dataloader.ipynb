{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN\n",
    "\n",
    "train_brain_signals_sub1 = torch.load('/GOD/subjectwise/train_brain_signals_sub1.pt')\n",
    "train_brain_signals_sub2 = torch.load('/GOD/subjectwise/train_brain_signals_sub2.pt')\n",
    "train_brain_signals_sub3 = torch.load('/GOD/subjectwise/train_brain_signals_sub3.pt')\n",
    "train_brain_signals_sub4 = torch.load('/GOD/subjectwise/train_brain_signals_sub4.pt')\n",
    "train_brain_signals_sub5 = torch.load('/GOD/subjectwise/train_brain_signals_sub5.pt')\n",
    "\n",
    "\n",
    "#TEST\n",
    "\n",
    "test_brain_signals_sub1 = torch.load('/GOD/subjectwise/test_brain_signals_sub1.pt')\n",
    "test_brain_signals_sub2 = torch.load('/GOD/subjectwise/test_brain_signals_sub2.pt')\n",
    "test_brain_signals_sub3 = torch.load('/GOD/subjectwise/test_brain_signals_sub3.pt')\n",
    "test_brain_signals_sub4 = torch.load('/GOD/subjectwise/test_brain_signals_sub4.pt')\n",
    "test_brain_signals_sub5 = torch.load('/GOD/subjectwise/test_brain_signals_sub5.pt')\n",
    "\n",
    "#TRAIN Images\n",
    "\n",
    "train_stimulus_sub1 = torch.load('/GOD/subjectwise/train_images_sub1.pt')\n",
    "train_stimulus_sub2 = torch.load('/GOD/subjectwise/train_images_sub2.pt')\n",
    "train_stimulus_sub3 = torch.load('/GOD/subjectwise/train_images_sub3.pt')\n",
    "train_stimulus_sub4 = torch.load('/GOD/subjectwise/train_images_sub4.pt')\n",
    "train_stimulus_sub5 = torch.load('/GOD/subjectwise/train_images_sub5.pt')\n",
    "\n",
    "#TEST Images\n",
    "\n",
    "test_stimulus_sub1 = torch.load('/GOD/subjectwise/test_images_sub1.pt')\n",
    "test_stimulus_sub2 = torch.load('/GOD/subjectwise/test_images_sub2.pt')\n",
    "test_stimulus_sub3 = torch.load('/GOD/subjectwise/test_images_sub3.pt')\n",
    "test_stimulus_sub4 = torch.load('/GOD/subjectwise/test_images_sub4.pt')\n",
    "test_stimulus_sub5 = torch.load('/GOD/subjectwise/test_images_sub5.pt')\n",
    "\n",
    "#TRAIN Brain Signals Imaginary\n",
    "\n",
    "train_brain_signals_imaginary_sub1 = torch.load('/GOD/subjectwise/train_brain_signals_imaginary_sub1.pt')\n",
    "train_brain_signals_imaginary_sub2 = torch.load('/GOD/subjectwise/train_brain_signals_imaginary_sub2.pt')\n",
    "train_brain_signals_imaginary_sub3 = torch.load('/GOD/subjectwise/train_brain_signals_imaginary_sub3.pt')\n",
    "train_brain_signals_imaginary_sub4 = torch.load('/GOD/subjectwise/train_brain_signals_imaginary_sub4.pt')\n",
    "train_brain_signals_imaginary_sub5 = torch.load('/GOD/subjectwise/train_brain_signals_imaginary_sub5.pt')\n",
    "\n",
    "#TEST Brain Signals Imaginary\n",
    "\n",
    "test_brain_signals_imaginary_sub1 = torch.load('/GOD/subjectwise/test_brain_signals_imaginary_sub1.pt')\n",
    "test_brain_signals_imaginary_sub2 = torch.load('/GOD/subjectwise/test_brain_signals_imaginary_sub2.pt')\n",
    "test_brain_signals_imaginary_sub3 = torch.load('/GOD/subjectwise/test_brain_signals_imaginary_sub3.pt')\n",
    "test_brain_signals_imaginary_sub4 = torch.load('/GOD/subjectwise/test_brain_signals_imaginary_sub4.pt')\n",
    "test_brain_signals_imaginary_sub5 = torch.load('/GOD/subjectwise/test_brain_signals_imaginary_sub5.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1200, 4466]),\n",
       " torch.Size([1200, 4404]),\n",
       " torch.Size([1200, 4643]),\n",
       " torch.Size([1200, 4133]),\n",
       " torch.Size([1200, 4370]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_brain_signals_sub1.shape, train_brain_signals_sub2.shape, train_brain_signals_sub3.shape, train_brain_signals_sub4.shape, train_brain_signals_sub5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50, 4466]),\n",
       " torch.Size([50, 4404]),\n",
       " torch.Size([50, 4643]),\n",
       " torch.Size([50, 4133]),\n",
       " torch.Size([50, 4370]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_brain_signals_sub1.shape, test_brain_signals_sub2.shape, test_brain_signals_sub3.shape, test_brain_signals_sub4.shape, test_brain_signals_sub5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1200, 3, 500, 500]),\n",
       " torch.Size([1200, 3, 500, 500]),\n",
       " torch.Size([1200, 3, 500, 500]),\n",
       " torch.Size([1200, 3, 500, 500]),\n",
       " torch.Size([1200, 3, 500, 500]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stimulus_sub1.shape, train_stimulus_sub2.shape, train_stimulus_sub3.shape, train_stimulus_sub4.shape, train_stimulus_sub5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50, 3, 500, 500]),\n",
       " torch.Size([50, 3, 500, 500]),\n",
       " torch.Size([50, 3, 500, 500]),\n",
       " torch.Size([50, 3, 500, 500]),\n",
       " torch.Size([50, 3, 500, 500]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_stimulus_sub1.shape, test_stimulus_sub2.shape, test_stimulus_sub3.shape, test_stimulus_sub4.shape, test_stimulus_sub5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([500, 4466]),\n",
       " torch.Size([500, 4404]),\n",
       " torch.Size([500, 4643]),\n",
       " torch.Size([500, 4133]),\n",
       " torch.Size([500, 4370]),\n",
       " torch.Size([50, 4466]),\n",
       " torch.Size([50, 4404]),\n",
       " torch.Size([50, 4643]),\n",
       " torch.Size([50, 4133]),\n",
       " torch.Size([50, 4370]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_brain_signals_imaginary_sub1.shape, train_brain_signals_imaginary_sub2.shape, train_brain_signals_imaginary_sub3.shape, train_brain_signals_imaginary_sub4.shape, train_brain_signals_imaginary_sub5.shape, test_brain_signals_imaginary_sub1.shape, test_brain_signals_imaginary_sub2.shape, test_brain_signals_imaginary_sub3.shape, test_brain_signals_imaginary_sub4.shape, test_brain_signals_imaginary_sub5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original length: 4466\n",
      "repeat_count: 3\n",
      "remainder: 2326\n",
      "Original length: 4404\n",
      "repeat_count: 3\n",
      "remainder: 2512\n",
      "Original length: 4643\n",
      "repeat_count: 3\n",
      "remainder: 1795\n",
      "Original length: 4133\n",
      "repeat_count: 3\n",
      "remainder: 3325\n",
      "Original length: 4370\n",
      "repeat_count: 3\n",
      "remainder: 2614\n"
     ]
    }
   ],
   "source": [
    "def repeat_and_pad(data, target_length=15724):\n",
    "    # Get the number of features (columns) in the data\n",
    "    original_length = data.shape[1]\n",
    "    print(f\"Original length: {original_length}\")\n",
    "    \n",
    "    # Repeat the data along the columns\n",
    "    repeat_count = target_length // original_length\n",
    "    print(f\"repeat_count: {repeat_count}\")\n",
    "    remainder = target_length % original_length\n",
    "    print(f\"remainder: {remainder}\")\n",
    "    \n",
    "    # Repeat and concatenate zeros if needed\n",
    "    repeated_data = np.tile(data, (1, repeat_count))  # Repeat columns\n",
    "    padded_data = np.pad(repeated_data, ((0, 0), (0, remainder)), mode='constant', constant_values=0)  # Pad with zeros\n",
    "    return padded_data\n",
    "\n",
    "\n",
    "target_length = 15724\n",
    "stacked_data = []\n",
    "\n",
    "train_brain_signals_imaginary_sub1 = repeat_and_pad(test_brain_signals_imaginary_sub1, target_length)\n",
    "train_brain_signals_imaginary_sub2 = repeat_and_pad(test_brain_signals_imaginary_sub2, target_length)\n",
    "train_brain_signals_imaginary_sub3 = repeat_and_pad(test_brain_signals_imaginary_sub3, target_length)\n",
    "train_brain_signals_imaginary_sub4 = repeat_and_pad(test_brain_signals_imaginary_sub4, target_length)\n",
    "train_brain_signals_imaginary_sub5 = repeat_and_pad(test_brain_signals_imaginary_sub5, target_length)\n",
    "\n",
    "stacked_tensor =  np.concatenate((train_brain_signals_imaginary_sub1, train_brain_signals_imaginary_sub2, train_brain_signals_imaginary_sub3, train_brain_signals_imaginary_sub4, train_brain_signals_imaginary_sub5), axis=0)\n",
    "\n",
    "\n",
    "stacked_tensor = torch.from_numpy(stacked_tensor)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(stacked_tensor, '/GOD/test_brain_signals_imaginary.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked tensor shape: torch.Size([250, 15724])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Stacked tensor shape: {stacked_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_brain_signal = torch.load('/GOD/train_brain_signals.pt')\n",
    "test_brain_signal = torch.load('/GOD/test_brain_signals.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6000, 15724]), torch.Size([250, 15724]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_brain_signal.shape, test_brain_signal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stimulus = torch.concatenate((train_stimulus_sub1, train_stimulus_sub2, train_stimulus_sub3, train_stimulus_sub4, train_stimulus_sub5), axis=0)\n",
    "test_stimulus = torch.concatenate((test_stimulus_sub1, test_stimulus_sub2, test_stimulus_sub3, test_stimulus_sub4, test_stimulus_sub5), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stimulus = torch.load('/GOD/train_stimulus.pt')\n",
    "test_stimulus = torch.load('/GOD/test_stimulus.pt')\n",
    "\n",
    "train_stimulus.shape, test_stimulus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c26bb1c725004e86aa32d0e9e64c3335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting to PIL: 100%|██████████| 250/250 [00:01<00:00, 201.28it/s]\n",
      "Processing images one by one: 100%|██████████| 250/250 [00:03<00:00, 66.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings with shape torch.Size([250, 2, 1, 1280]) to test_image_embeddings.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import AutoPipelineForImage2Image\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "device = \"cuda:3\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "pipeline = AutoPipelineForImage2Image.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    torch_dtype=torch.float16\n",
    ").to(device)\n",
    "pipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=\"ip-adapter_sdxl.bin\")\n",
    "pipeline.set_ip_adapter_scale(0.6)\n",
    "\n",
    "# Load your images tensor (shape: [18870, 3, 224, 224])\n",
    "images_tensor = test_stimulus\n",
    "num_images = images_tensor.shape[0]\n",
    "\n",
    "# Convert tensor to list of PIL Images (adjust scaling if needed)\n",
    "pil_images = []\n",
    "for i in tqdm(range(num_images), desc=\"Converting to PIL\"):\n",
    "    img_tensor = images_tensor[i].permute(1, 2, 0)  # [224, 224, 3]\n",
    "    \n",
    "    # If tensor is in [0,1], scale to 0-255. Adjust if different.\n",
    "    if img_tensor.max() <= 1.0:\n",
    "        img_tensor = (img_tensor * 255).clamp(0, 255)\n",
    "    img_tensor = img_tensor.to(torch.uint8).cpu().numpy()\n",
    "    \n",
    "    pil_images.append(Image.fromarray(img_tensor))\n",
    "\n",
    "# Process images one by one to extract embeddings\n",
    "all_embeddings = []\n",
    "for i in tqdm(range(num_images), desc=\"Processing images one by one\"):\n",
    "    image = pil_images[i:i+1]  # Select one image at a time\n",
    "    \n",
    "    # Get embeddings for current image\n",
    "    with torch.inference_mode():\n",
    "        embeddings = pipeline.prepare_ip_adapter_image_embeds(\n",
    "            ip_adapter_image=image,\n",
    "            ip_adapter_image_embeds=None,\n",
    "            device=device,\n",
    "            num_images_per_prompt=1,\n",
    "            do_classifier_free_guidance=True  \n",
    "        )\n",
    "    \n",
    "    all_embeddings.append(embeddings[0])\n",
    "\n",
    "# Concatenate and save\n",
    "embeddings_tensor = torch.stack(all_embeddings, dim=0)\n",
    "torch.save(embeddings_tensor, \"GOD/test_image_embeddings.pt\")\n",
    "\n",
    "print(f\"Saved embeddings with shape {embeddings_tensor.shape} to test_image_embeddings.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "subject_ids = []\n",
    "\n",
    "\n",
    "subject_ids.extend([12] * 50)  \n",
    "subject_ids.extend([13] * 50)  \n",
    "subject_ids.extend([14] * 50) \n",
    "subject_ids.extend([15] * 50) \n",
    "subject_ids.extend([16] * 50)\n",
    "\n",
    "\n",
    "test_subject_ids_tensor = torch.tensor(subject_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([250])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_subject_ids_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(test_subject_ids_tensor, '/GOD/test_subject_ids.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
